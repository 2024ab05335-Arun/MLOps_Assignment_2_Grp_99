{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a56345",
   "metadata": {},
   "source": [
    "#### MLOps_Assignment_2_Grp_99\n",
    "- 1. Arun Gupta (2024AB05335)\n",
    "- 2. Ankur Gupta (2024AB05337)\n",
    "- 3. Arjun Chandran (2024AB05336)\n",
    "- 4. Vineet Suresh Kumar Yadav (2024AB05340)\n",
    "- 5. Umang Nathani (2024AB05334)\n",
    "\n",
    "##### Data pre-processing and spiliting.\n",
    "- Pre-process to 224x224 RGB images for standard CNNs. Split into train/validation/test sets (e.g., 80%/10%/10%).\n",
    "    \n",
    "    Files added:\n",
    "    - `preprocess_petimages.py` — main preprocessing script\n",
    "    - `data_split.py` — data split script\n",
    "\n",
    "    Quick run:\n",
    "\n",
    "        ```bash\n",
    "        python preprocess_petimages.py \\\n",
    "        --input data/Dataset/PetImages \\\n",
    "        --output data/processed/PetImages_224 \\\n",
    "        --method resize_crop \\\n",
    "        --workers 8\n",
    "        ```\n",
    "\n",
    "        ```bash\n",
    "        python data_split.py\n",
    "        ```\n",
    "\n",
    "##### M1:  Model Development & Experiment Tracking\n",
    "- 1. Data & Code Versioning\n",
    "\n",
    "        Use Git for source code versioning and Git‑LFS for dataset versioning and to track pre-processed data.\n",
    "        File added to track data versioning:\n",
    "        - `.gitattributes`\n",
    "- 2. Model Building \n",
    "\n",
    "        Use simple CNN for model training and save the trained model in .keras format.\n",
    "        File updated to track models(due to large model size):\n",
    "        - `.gitattributes`\n",
    "- 3. Experiment Tracking\n",
    "\n",
    "        Use MLFlow to log runs, parameters, metrics, and artifacts (confusion matrix, loss curves)\n",
    "        \n",
    "        Files added:\n",
    "\n",
    "        - `mlflow_workflow.py` — Train model and configure MLFlow\n",
    "            ```bash\n",
    "            python mlflow_workflow.py\n",
    "            python mlflow_workflow.py --epochs 10\n",
    "            mlflow ui --backend-store-uri mlruns --port 5000\n",
    "            ```\n",
    "\n",
    "        MLFlow UI screenshot added in folder:\n",
    "        ```\n",
    "        mlflow_screenshots\n",
    "        ```\n",
    "\n",
    "##### M2: Model Packaging & Containerization \n",
    "- 1. Inference Service\n",
    "\n",
    "        Wrap the trained model with a simple REST API using FastAPI. \n",
    "\n",
    "        Files added:\n",
    "        - `inference_service.py` — REST API to access model for inderence\n",
    "        - `inference_client.py` — Inference client to validate the image\n",
    "\n",
    "            ```bash\n",
    "            python inference_service.py --port 8000 --model-uri \"runs:/<run_id>/model\"\n",
    "            python inference_service.py --port 8000 --model-uri \"runs:/a40b43613cdb458b93c2da1e0a723b02/model\"\n",
    "            python inference_client.py --image /path/to/image.jpg --mode file --url http://localhost:8000\n",
    "            python inference_client.py --image /path/to/image.jpg --mode base64 --url http://localhost:8000\n",
    "            ```\n",
    "- 2. Environment Specification\n",
    "        - `requirements.txt`\n",
    "- 3. Containerization:\n",
    "        - `Dockerfile`\n",
    "        - `download_model.py`\n",
    "        - `entrypoint.sh`\n",
    "        - build image\n",
    "            ```\n",
    "            docker build -t pet-inference:latest .\n",
    "            ```\n",
    "        - run using an MLflow model URI (recommended)\n",
    "            ```\n",
    "            docker run -p 8000:8000 --rm -e MODEL_URI=\"runs:/<run_id>/model\" pet-inference:latest\n",
    "            ```\n",
    "##### M3: CI Pipeline for Build, Test & Image Creation\n",
    "- 1. Automated Testing\n",
    "\n",
    "        Unit test using pytest\n",
    "\n",
    "        Files added:\n",
    "        - `tests/test_inference_utils.py` — REST API to access model for inderence\n",
    "        - `tests/test_preprocess.py` — Inference client to validate the image\n",
    "\n",
    "            ```bash\n",
    "            python -m pytest -q tests -q\n",
    "            python -m pytest tests -vv > pytest_output.txt 2>&1\n",
    "            ```\n",
    "- 2. CI Setup (Choose one: GitHub Actions / GitLab CI / Jenkins / Tekton) \n",
    "\n",
    "        A GitHub Actions workflow has been added to run tests and build the Docker image:\n",
    "\n",
    "        Workflow files:\n",
    "        - `.github/workflows/ci.yml` (root) — generic repo CI\n",
    "        - `MLOps_Assignment_2_Grp_99/.github/workflows/ci.yml` — runs tests and builds the subproject image\n",
    "- 3. Artifact Publishing: \n",
    "\n",
    "        To enable pushing built images from CI, set one of the following repository secrets:\n",
    "\n",
    "        - GitHub Container Registry (recommended): the workflow uses `GITHUB_TOKEN` but requires `packages: write` permission (already set in workflow).\n",
    "        - For Docker Hub: set `DOCKERHUB_USERNAME` and `DOCKERHUB_PASSWORD` in repository Secrets.\n",
    "\n",
    "            Local test / build commands:\n",
    "            ```bash\n",
    "            # Run unit tests locally\n",
    "            python -m pytest -q MLOps_Assignment_2_Grp_99/tests\n",
    "            # Build the Docker image locally (from repo root)\n",
    "            docker build -t mlops-assignment2/pet-inference:latest -f MLOps_Assignment_2_Grp_99/Dockerfile MLOps_Assignment_2_Grp_99\n",
    "            # Push to Docker Hub (optional)\n",
    "            docker tag mlops-assignment2/pet-inference:latest <your-dockerhub-username>/mlops-assignment2:latest\n",
    "            docker push <your-dockerhub-username>/mlops-assignment2:latest\n",
    "            ```\n",
    "\n",
    "##### M4: CD Pipeline & Deployment    \n",
    "- 1. Deployment Target \n",
    "    \n",
    "        A `docker-compose.yml` is provided to run the inference service and an optional MLflow UI. \n",
    "        \n",
    "        To keep secrets out of source control, copy `.env.example` to `.env` and update values:\n",
    "\n",
    "        ```bash\n",
    "        cp MLOps_Assignment_2_Grp_99/.env.example MLOps_Assignment_2_Grp_99/.env\n",
    "        # edit MLOps_Assignment_2_Grp_99/.env then start services\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml up --build -d\n",
    "        # view logs\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml logs -f\n",
    "        # stop services\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml down\n",
    "        ```\n",
    "- 2. CD / GitOps Flow \n",
    "\n",
    "        A GitHub Actions workflow has been added to automatically deploy the updated image to a remote server when `main` is updated.\n",
    "\n",
    "        Workflow file: `MLOps_Assignment_2_Grp_99/.github/workflows/cd.yml`\n",
    "\n",
    "        What the workflow does:\n",
    "        - Runs on `push` to `main` and targets the `production` environment.\n",
    "        - Uses an SSH private key (stored in `DEPLOY_SSH_KEY`) to connect to the deployment host.\n",
    "        - Optionally logs in to Docker Hub on the remote host (if `DOCKERHUB_USERNAME`/`DOCKERHUB_PASSWORD` are set), then runs `docker-compose pull` and `docker-compose up -d` in the deployment path.\n",
    "        - Verifies the inference health endpoint after deployment.\n",
    "\n",
    "        Repository secrets required (set these in Settings → Secrets → Actions):\n",
    "        - `DEPLOY_HOST`: the IP or hostname of the target server\n",
    "        - `DEPLOY_USER`: SSH user on the server\n",
    "        - `DEPLOY_PATH`: path on the server where `docker-compose.yml` lives (e.g., `/srv/pet-inference`)\n",
    "        - `DEPLOY_SSH_KEY`: private SSH key (PEM) for `DEPLOY_USER`\n",
    "        - `DEPLOY_SSH_PORT`: optional (default 22)\n",
    "\n",
    "        Manual deploy test commands (on the target host):\n",
    "\n",
    "        ```bash\n",
    "        # login (if needed)\n",
    "        docker login -u <user> -p '<password>'\n",
    "        # update and restart with docker-compose (run in DEPLOY_PATH)\n",
    "        docker-compose pull\n",
    "        docker-compose up -d --remove-orphans\n",
    "        # check health locally on host\n",
    "        curl http://localhost:8000/health\n",
    "        ```\n",
    "- 3. Smoke Tests / Health Check \n",
    "\n",
    "        - Verify endpoints:\n",
    "\n",
    "            ```bash\n",
    "            curl http://localhost:8000/health\n",
    "            curl -X POST \"http://localhost:8000/predict\" -F \"files=@data/processed/PetImages_224_split/test/Cat/<image>.jpg\"\n",
    "            ```\n",
    "\n",
    "##### M5: Monitoring, Logs & Final Submission    \n",
    "- 1. Basic Monitoring & Logging \n",
    "\n",
    "        Logging added to inference service and logs are created in:\n",
    "        - `inference.log`\n",
    "- 2. Model Performance Tracking (Post‑Deployment) \n",
    "\n",
    "        A small helper script is provided to collect a batch of requests (images) against the running inference service and store the responses together with true labels for later analysis.\n",
    "\n",
    "        Script: `monitoring/collect_requests.py`\n",
    "\n",
    "        Usage example (from repo root):\n",
    "        ```bash\n",
    "        # collect up to 100 images per class from the test split and post to local service\n",
    "        python monitoring/collect_requests.py --url http://localhost:8000 --image-dir data/processed/PetImages_224_split/test --output monitoring/results.csv --limit 100\n",
    "        ```\n",
    "\n",
    "        Output: CSV (`monitoring/results.csv`) with columns: `image_path`, `true_label`, `status_code`, `predicted_label`, `response_json`. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
