{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a56345",
   "metadata": {},
   "source": [
    "#### MLOps_Assignment_2_Grp_99\n",
    "- 1. Arun Gupta (2024AB05335)\n",
    "- 2. Ankur Gupta (2024AB05337)\n",
    "- 3. Arjun Chandran (2024AB05336)\n",
    "- 4. Vineet Suresh Kumar Yadav (2024AB05340)\n",
    "- 5. Umang Nathani (2024AB05334)\n",
    "\n",
    "##### Data pre-processing and spiliting.\n",
    "- Pre-process to 224x224 RGB images for standard CNNs. Split into train/validation/test sets (e.g., 80%/10%/10%).\n",
    "    \n",
    "    Files added:\n",
    "    - `preprocess_petimages.py` — main preprocessing script\n",
    "    - `data_split.py` — data split script\n",
    "\n",
    "    - Quick run:\n",
    "\n",
    "        ```bash\n",
    "        python preprocess_petimages.py --input data/Dataset/PetImages --output data/processed/PetImages_224 --method resize_crop --workers 8\n",
    "        python data_split.py\n",
    "        ```\n",
    "\n",
    "##### M1:  Model Development & Experiment Tracking\n",
    "- 1. Data & Code Versioning\n",
    "\n",
    "        Use Git for source code versioning and Git‑LFS for dataset versioning and to track pre-processed data.\n",
    "        File added to track data versioning:\n",
    "        - `.gitattributes`\n",
    "- 2. Model Building \n",
    "\n",
    "        Use simple CNN for model training and save the trained model in .keras format.\n",
    "        File updated to track models(due to large model size):\n",
    "        - `.gitattributes`\n",
    "- 3. Experiment Tracking\n",
    "\n",
    "        Use MLFlow to log runs, parameters, metrics, and artifacts (confusion matrix, loss curves)\n",
    "        \n",
    "        Files added:\n",
    "\n",
    "        - `mlflow_workflow.py` — Train model and configure MLFlow\n",
    "\n",
    "            ```bash\n",
    "            python mlflow_workflow.py\n",
    "            python mlflow_workflow.py --epochs 10\n",
    "            mlflow ui --backend-store-uri mlruns --port 5000\n",
    "            ```\n",
    "\n",
    "        MLFlow UI screenshot added in folder:\n",
    "        ```\n",
    "        mlflow_screenshots\n",
    "        ```\n",
    "\n",
    "##### M2: Model Packaging & Containerization \n",
    "- 1. Inference Service\n",
    "\n",
    "        Wrap the trained model with a simple REST API using FastAPI. \n",
    "\n",
    "        Files added:\n",
    "        - `inference_service.py` — REST API to access model for inderence\n",
    "        - `inference_client.py` — Inference client to validate the image\n",
    "\n",
    "            ```bash\n",
    "            python inference_service.py --port 8000 --model-uri \"runs:/<run_id>/model\"\n",
    "            python inference_service.py --port 8000 --model-uri \"runs:/a40b43613cdb458b93c2da1e0a723b02/model\"\n",
    "            python inference_client.py --image /path/to/image.jpg --mode file --url http://localhost:8000\n",
    "            python inference_client.py --image /path/to/image.jpg --mode base64 --url http://localhost:8000\n",
    "            ```\n",
    "- 2. Environment Specification\n",
    "        - `requirements.txt`\n",
    "- 3. Containerization:\n",
    "        - `Dockerfile`\n",
    "        - `download_model.py`\n",
    "        - `entrypoint.sh`\n",
    "        - build image\n",
    "            ```\n",
    "            docker build -t pet-inference:latest .\n",
    "            ```\n",
    "        - run using an MLflow model URI (recommended)\n",
    "            ```\n",
    "            docker run -p 8000:8000 --rm -e MODEL_URI=\"runs:/<run_id>/model\" pet-inference:latest\n",
    "            ```\n",
    "##### M3: CI Pipeline for Build, Test & Image Creation\n",
    "- 1. Automated Testing\n",
    "\n",
    "        Unit test using pytest\n",
    "\n",
    "        Files added:\n",
    "        - `tests/test_inference_utils.py` — REST API to access model for inference\n",
    "        - `tests/test_preprocess.py` — Inference client to validate the image\n",
    "\n",
    "            ```bash\n",
    "            python -m pytest -q tests -q\n",
    "            python -m pytest tests -vv > pytest_output.txt 2>&1\n",
    "            ```\n",
    "- 2. CI Setup (Choose one: GitHub Actions / GitLab CI / Jenkins / Tekton) \n",
    "\n",
    "        A GitHub Actions workflow has been added to run tests and build the Docker image:\n",
    "\n",
    "        Workflow files:\n",
    "        - `.github/workflows/ci.yml` (root) — generic repo CI\n",
    "        - `.github/workflows/ci.yml` — runs tests and builds the subproject image\n",
    "- 3. Artifact Publishing: \n",
    "\n",
    "        To enable pushing built images from CI, set one of the following repository secrets:\n",
    "\n",
    "        - GitHub Container Registry (recommended): the workflow uses `GITHUB_TOKEN` but requires `packages: write` permission (already set in workflow).\n",
    "        - For Docker Hub: set `DOCKERHUB_USERNAME` and `DOCKERHUB_PASSWORD` in repository Secrets.\n",
    "\n",
    "            Local test / build commands:\n",
    "            ```bash\n",
    "            # Run unit tests locally\n",
    "            python -m pytest -q MLOps_Assignment_2_Grp_99/tests\n",
    "            # Build the Docker image locally (from repo root)\n",
    "            docker build -t mlops-assignment2/pet-inference:latest -f MLOps_Assignment_2_Grp_99/Dockerfile MLOps_Assignment_2_Grp_99\n",
    "            # Push to Docker Hub (optional)\n",
    "            docker tag mlops-assignment2/pet-inference:latest <your-dockerhub-username>/mlops-assignment2:latest\n",
    "            docker push <your-dockerhub-username>/mlops-assignment2:latest\n",
    "            ```\n",
    "\n",
    "##### M4: CD Pipeline & Deployment    \n",
    "- 1. Deployment Target \n",
    "    \n",
    "        A `docker-compose.yml` is provided to run the inference service and an optional MLflow UI. \n",
    "        \n",
    "        To keep secrets out of source control, copy `.env.example` to `.env` and update values:\n",
    "\n",
    "        ```bash\n",
    "        cp MLOps_Assignment_2_Grp_99/.env.example MLOps_Assignment_2_Grp_99/.env\n",
    "        # edit MLOps_Assignment_2_Grp_99/.env then start services\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml up --build -d\n",
    "        # view logs\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml logs -f\n",
    "        # stop services\n",
    "        docker-compose -f MLOps_Assignment_2_Grp_99/docker-compose.yml down\n",
    "        ```\n",
    "- 2. CD / GitOps Flow \n",
    "\n",
    "        A GitHub Actions workflow has been added to automatically deploy the updated image to a remote server when `main` is updated.\n",
    "\n",
    "        Workflow file: `MLOps_Assignment_2_Grp_99/.github/workflows/cd.yml`\n",
    "\n",
    "        What the workflow does:\n",
    "        - Runs on `push` to `main` and targets the `production` environment.\n",
    "        - Uses an SSH private key (stored in `DEPLOY_SSH_KEY`) to connect to the deployment host.\n",
    "        - Optionally logs in to Docker Hub on the remote host (if `DOCKERHUB_USERNAME`/`DOCKERHUB_PASSWORD` are set), then runs `docker-compose pull` and `docker-compose up -d` in the deployment path.\n",
    "        - Verifies the inference health endpoint after deployment.\n",
    "\n",
    "        Repository secrets required (set these in Settings → Secrets → Actions):\n",
    "        - `DEPLOY_HOST`: the IP or hostname of the target server\n",
    "        - `DEPLOY_USER`: SSH user on the server\n",
    "        - `DEPLOY_PATH`: path on the server where `docker-compose.yml` lives (e.g., `/srv/pet-inference`)\n",
    "        - `DEPLOY_SSH_KEY`: private SSH key (PEM) for `DEPLOY_USER`\n",
    "        - `DEPLOY_SSH_PORT`: optional (default 22)\n",
    "\n",
    "        Manual deploy test commands (on the target host):\n",
    "\n",
    "        ```bash\n",
    "        # login (if needed)\n",
    "        docker login -u <user> -p '<password>'\n",
    "        # update and restart with docker-compose (run in DEPLOY_PATH)\n",
    "        docker-compose pull\n",
    "        docker-compose up -d --remove-orphans\n",
    "        # check health locally on host\n",
    "        curl http://localhost:8000/health\n",
    "        ```\n",
    "- 3. Smoke Tests / Health Check \n",
    "\n",
    "        - Verify endpoints:\n",
    "\n",
    "            ```bash\n",
    "            curl http://localhost:8000/health\n",
    "            curl -X POST \"http://localhost:8000/predict\" -F \"files=@data/processed/PetImages_224_split/test/Cat/<image>.jpg\"\n",
    "            ```\n",
    "\n",
    "##### M5: Monitoring, Logs & Final Submission    \n",
    "- 1. Basic Monitoring & Logging \n",
    "\n",
    "        Logging added to inference service and logs are created in:\n",
    "        - `inference.log`\n",
    "- 2. Model Performance Tracking (Post‑Deployment) \n",
    "\n",
    "        A small helper script is provided to collect a batch of requests (images) against the running inference service and store the responses together with true labels for later analysis.\n",
    "\n",
    "        Script: `monitoring/collect_requests.py`\n",
    "\n",
    "        Usage example (from repo root):\n",
    "        ```bash\n",
    "        # collect up to 100 images per class from the test split and post to local service\n",
    "        python monitoring/collect_requests.py --url http://localhost:8000 --image-dir data/processed/PetImages_224_split/test --output monitoring/results.csv --limit 100\n",
    "        ```\n",
    "\n",
    "        Output: CSV (`monitoring/results.csv`) with columns: `image_path`, `true_label`, `status_code`, `predicted_label`, `response_json`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b62a2",
   "metadata": {},
   "source": [
    "##### Sample Run\n",
    "\n",
    "- Start MLFlow UI:\n",
    "\n",
    "    ```bash\n",
    "        mlflow ui --backend-store-uri file:///C:/ArunDocs/Code/MLOps_Assignment_2_Grp_99/mlruns --port 5000\n",
    "    ```\n",
    "\n",
    "- Start Inference Service for model with run_id: ef566c44824241769ef049f8c0f15577\n",
    "\n",
    "    ```bash\n",
    "        python inference_service.py --port 8000 --model-uri \"runs:/ef566c44824241769ef049f8c0f15577/model\"\n",
    "    ```\n",
    "        \n",
    "- Check health of inference service\n",
    "\n",
    "    ```bash\n",
    "        localhost:8000/metrics\n",
    "    ```\n",
    "    - Output\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"request_count\": 215,\n",
    "            \"avg_latency_ms\": 89.6065323851829\n",
    "        }\n",
    "\n",
    "    ```bash\n",
    "        localhost:8000/health\n",
    "    ```\n",
    "    - Output\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"status\": \"ok\",\n",
    "            \"model_loaded\": true\n",
    "        }\n",
    "\n",
    "- Test client for inference service\n",
    "\n",
    "    ```bash\n",
    "        python inference_client.py --image data\\processed\\PetImages_224_split\\val\\Cat\\2.jpg --mode file --url http://localhost:8000\n",
    "    ```\n",
    "    - Output : Predict: Cat (Class 0)\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                \"probabilities\": [\n",
    "                    0.8913493156433105,\n",
    "                    0.10865061730146408\n",
    "                ],\n",
    "                \"predicted_class\": 0,\n",
    "                \"predicted_label\": null\n",
    "                }\n",
    "            ]\n",
    "        } \n",
    "\n",
    "    ```bash\n",
    "        python inference_client.py --image C:\\ArunDocs\\Code\\MLOps_Assignment_2_Grp_99\\data\\processed\\PetImages_224_split\\val\\Dog\\5.jpg --mode file --url http://localhost:8000\n",
    "    ```\n",
    "    - Output : Predict: Dog (Class 1)\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                \"probabilities\": [\n",
    "                    0.009738803841173649,\n",
    "                    0.9902612566947937\n",
    "                ],\n",
    "                \"predicted_class\": 1,\n",
    "                \"predicted_label\": null\n",
    "                }\n",
    "            ]\n",
    "        } \n",
    "\n",
    "- Test Inference Service\n",
    "    ```bash\n",
    "        curl http://localhost:8000/health\n",
    "    ```\n",
    "    - Output:\n",
    "\n",
    "        ```\n",
    "        {\"status\":\"ok\",\"model_loaded\":true}\n",
    "    ```bash\n",
    "        curl -X POST \"http://localhost:8000/predict\" -F \"files=@data/processed/PetImages_224_split/test/Cat/111.jpg\"\n",
    "    ```\n",
    "    - Output:\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                \"probabilities\": [\n",
    "                    0.9991834759712219,\n",
    "                    0.0008165583130903542\n",
    "                ],\n",
    "                \"predicted_class\": 0,\n",
    "                \"predicted_label\": null\n",
    "                }\n",
    "            ]\n",
    "        } \n",
    "    ```bash\n",
    "        curl -X POST \"http://localhost:8000/predict\" -F \"files=@data/processed/PetImages_224_split/test/Dog/0.jpg\"\n",
    "    ```\n",
    "\n",
    "    - Output:\n",
    "\n",
    "        ```\n",
    "        {\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                \"probabilities\": [\n",
    "                    0.005587819032371044,\n",
    "                    0.9944122433662415\n",
    "                ],\n",
    "                \"predicted_class\": 1,\n",
    "                \"predicted_label\": null\n",
    "                }\n",
    "            ]\n",
    "        } \n",
    "\n",
    "- Test inference service logging\n",
    "\n",
    "    ```\n",
    "    2026-02-22 01:24:19,363 INFO req_id=a447e59e-6cea-4655-8160-bb50e0a1e72c method=GET path=/metrics latency_ms=5.0\n",
    "    2026-02-22 01:24:19,398 INFO req_id=82be4b24-1802-47ee-9f83-f93879f2c764 method=GET path=/favicon.ico latency_ms=0.5\n",
    "    2026-02-22 01:24:19,398 INFO req_id=82be4b24-1802-47ee-9f83-f93879f2c764 method=GET path=/favicon.ico latency_ms=0.5\n",
    "    2026-02-22 01:25:32,632 INFO req_id=56224d28-f164-495f-90c4-49ce90c701a2 method=GET path=/predict latency_ms=0.4\n",
    "    2026-02-22 01:25:32,632 INFO req_id=56224d28-f164-495f-90c4-49ce90c701a2 method=GET path=/predict latency_ms=0.4\n",
    "    2026-02-22 01:25:39,700 INFO req_id=df470e54-150b-4a2c-86e3-feaf1f1d8504 method=GET path=/health latency_ms=1.4\n",
    "    2026-02-22 01:25:39,700 INFO req_id=df470e54-150b-4a2c-86e3-feaf1f1d8504 method=GET path=/health latency_ms=1.4\n",
    "    2026-02-22 01:27:41,583 INFO predict num_instances=1\n",
    "    2026-02-22 01:27:41,583 INFO predict num_instances=1\n",
    "    2026-02-22 01:27:41,584 INFO req_id=c61390a4-ffe7-45b9-a890-ffec3b752507 method=POST path=/predict latency_ms=472.9\n",
    "    2026-02-22 01:27:41,584 INFO req_id=c61390a4-ffe7-45b9-a890-ffec3b752507 method=POST path=/predict latency_ms=472.9\n",
    "    2026-02-22 01:29:41,434 INFO predict num_instances=1\n",
    "    2026-02-22 01:29:41,434 INFO predict num_instances=1\n",
    "    2026-02-22 01:29:41,435 INFO req_id=ddb55258-7b63-4bae-975b-01b37cbdba81 method=POST path=/predict latency_ms=98.3\n",
    "    2026-02-22 01:29:41,435 INFO req_id=ddb55258-7b63-4bae-975b-01b37cbdba81 method=POST path=/predict latency_ms=98.3\n",
    "    2026-02-22 01:39:02,327 INFO req_id=049787ea-b6b9-4894-9892-9931d8157749 method=GET path=/health latency_ms=0.8\n",
    "    2026-02-22 01:39:02,327 INFO req_id=049787ea-b6b9-4894-9892-9931d8157749 method=GET path=/health latency_ms=0.8\n",
    "    2026-02-22 01:39:31,310 INFO req_id=18b0b3da-8c4e-4eb3-a268-dcf3757128c3 method=GET path=/healthsource latency_ms=0.3\n",
    "    2026-02-22 01:39:31,310 INFO req_id=18b0b3da-8c4e-4eb3-a268-dcf3757128c3 method=GET path=/healthsource latency_ms=0.3\n",
    "    2026-02-22 01:39:55,849 INFO req_id=b37b5931-d088-4f31-b7da-88572512b4b9 method=GET path=/health latency_ms=0.8\n",
    "\n",
    "- Test model performance script\n",
    "\n",
    "    ```bash\n",
    "        python monitoring/collect_requests.py --url http://localhost:8000 --image-dir data/processed/PetImages_224_split/test --output monitoring/results.csv --limit 100\n",
    "    ```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
